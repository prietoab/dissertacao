\chapter{Conclusão}
\label{cap:conclusao}

	A epígrafe dessa dissertação cita a capacidade das pessoas seguirem ``\emph{longas cadeias de raciocínio contruídas com elos cuja verdade elas não observaram diretamente}'', e que a ciência é possível justamente por esse tipo de confiança. Baseado nos argumentos do artigo \cite{metodo2004}, confiei que, ao reproduzir seu método, eu obteria o autovalor mínimo de matrizes simétricas. Para minha surpresa, concluí, pelo contrário, que seguir \cite{metodo2004} não garante o mínimo global, mas mínimos locais.
	
	A função de avaliação $f_i = e^{-\beta |\nabla \rho_i|^2}$ depende apenas do gradiente do Quociente de Rayleigh [$f_i = g(\nabla \rho_i)$] que, quando nulo ($\nabla \rho = \textbf{0}$), indica que encontramos \emph{algum} dos autovalores, e não necessariamente o menor, como afirmaram os autores.
	
	Concluí que essa impossibilidade de encontrar o autovalor mínimo de matrizes simétricas com \cite{metodo2004} não reside em uma falha do método em si, mas apenas na má definição da função de avaliação. De fato, usando o mesmo Algoritmo Genético (GA) de \cite{metodo2004}, mas com o \emph{fitness} $f_i = e^{-\beta(\rho - E_L)^2}$ de \cite{metodo2011}, foi possível, com bom ajuste do novo parâmetro $E_L$, encontrar \emph{sempre} o mínimo. Em especial, afirmo que é impossível, para esse método, obter \emph{sempre} o menor autovalor com um \emph{fitness} que dependa apenas de $\nabla \rho$.

	As duas funções de avaliação podem ser utilizadas em conjunto. A de \cite{metodo2011} encontra o autovalor mínimo, porém, é necessário conhecimento prévio sobre a região onde ele se encontra, assim como a configuração de dois parâmetros ($\beta$ e $E_L$). Apesar de não chegar ao autovalor mínimo, o \emph{fitness} de \cite{metodo2004} é mais preciso e necessita de apenas um parâmetro ($\beta$). Além disso, ele identifica diferentes autovalores intermediários, e essa informação pode ser útil para auxiliar na definição de uma boa região para o \emph{fitness} de \cite{metodo2011}.
	
	A configuração do parâmetro $\beta$ merece cuidado, mas os criadores do método não deram detalhes sobre como determiná-lo. Descobri que ele deve ser escolhido de modo que as funções de avaliação (gaussianas) sejam estreitas, punindo muito os piores indivíduos, mas ao mesmo tempo garantindo que esses, na população inicial, tenham \emph{fitness} levemente superiores à zero. Observando simetrias da matriz de Coope--Sabo, propus uma equação para $\beta$ que depende apenas da ordem da matriz. Apesar de eu ter partido do \emph{fitness} de \cite{metodo2011}, após um pequeno ajuste a equação mostrou-se válida também para \cite{metodo2004}. Isso permite que a obtenção de $\beta$ para aquelas matrizes de teste seja automática.
	
	Portanto, o método apresentado nos artigos \cite{metodo2004} e \cite{metodo2011}, que transforma o cálculo de autovalores de matrizes simétricas em um problema de Otimização Combinatória por meio de GAs, realmente funciona. No entanto, há base para contradizer, parcialmente, \cite{metodo2004}.
	
	Apesar de não ter apresentado neste trabalho uma análise formal de desempenho do meu programa, percebi que ele não é competitivo com o Scilab do ponto de vista de tempo de processamento. Entretanto, há possibilidade de reduzir esse custo computacional para matrizes grandes paralelizando o código em unidades de processamento gráfico (GPUs, veja o apêndice \ref{apdx:oneMaxNaGPU}).